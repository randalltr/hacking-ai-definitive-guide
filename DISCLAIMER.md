# Disclaimer

**Hacking AI: The Definitive Guide** is a technical, adversarial AI security resource created for **educational, academic, and ethical research purposes only**. It exists to improve awareness, resilience, and robustness of large language model (LLM) systems—not to facilitate or endorse misuse.

By reading, downloading, referencing, cloning, distributing, or otherwise using this material, **you agree to the following conditions**.

---

## 1. No Unauthorized Use

This book does **not** grant you permission to test, exploit, or interfere with any LLM-based system, service, or platform that you do not own or operate, or for which you do not have **explicit, written authorization** to perform security research.

Examples of unauthorized targets include (but are not limited to):

- Public commercial APIs (e.g., OpenAI, Anthropic, Google, Meta, Microsoft)
- Hosted web applications using embedded AI agents or chatbots
- SaaS platforms with integrated LLM components
- Third-party products, services, or deployments

Engaging in unauthorized testing may violate local or international law, including but not limited to:

- **Computer Fraud and Abuse Act (CFAA)**  
- **Digital Millennium Copyright Act (DMCA)**  
- **UK Computer Misuse Act**  
- **Terms of Service of the provider or application**  

**You are solely and fully responsible for your actions.**

---

## 2. Intended Use

This book is intended for:

- **Red team training and education**
- **Security research under responsible disclosure**
- **Academic instruction in adversarial machine learning**
- **Professional development in AI alignment and safety auditing**
- **Analysis and documentation of real-world LLM vulnerabilities**

None of the content is to be interpreted as an endorsement or encouragement of malicious behavior, criminal activity, unauthorized access, surveillance, impersonation, or any form of ethical misconduct.

---

## 3. No Warranty or Liability

This material is provided **"as is"** without warranty of any kind—explicit or implied. The author(s), contributors, and publisher(s) shall not be held liable for:

- Any damages or losses resulting from the use or misuse of the information
- Legal or regulatory consequences incurred by any user
- Business, platform, or provider disruptions due to improper testing
- Third-party violations of acceptable use or security policies

Use this material **at your own risk**, and consult your legal counsel and internal policies before conducting any form of adversarial testing.

---

## 4. No Guarantee of Results

AI models are non-deterministic. Techniques described in this book:

- May work today but fail tomorrow
- May succeed against some models and fail against others
- Are likely to be detected, filtered, or patched by providers
- Depend on highly specific prompt structures, context lengths, model tuning, and application logic

This book does **not guarantee any exploit will work** against any given model at any time.

The goal is **methodological understanding**, not tactical certainty.

---

## 5. Respect Intellectual Property

All model names (e.g., GPT-4, Claude, LLaMA), company names, and product references are the trademarks of their respective owners. Their use in this book is for **descriptive and research purposes only** and does not imply endorsement or affiliation.

No attempt is made to reverse-engineer proprietary systems beyond legally accepted boundaries of academic and security research.

---

## 6. Ethics First

You are expected to:

- Follow responsible disclosure procedures when identifying real-world vulnerabilities
- Respect provider terms of use and security testing guidelines
- Operate under permission, consent, and signed authorization when conducting red team assessments
- Uphold professional codes of conduct as a penetration tester, engineer, educator, or researcher

> If you are unsure whether something is legal or ethical—**do not do it**.

---

## Final Note

**Hacking AI: The Definitive Guide** exists to build safer systems—not break them for personal gain.

This book teaches you how to test, not how to harm.  
To discover flaws, not to exploit users.  
To challenge assumptions, not to dismantle trust.

Be informed. Be rigorous. Be responsible.

**If you do not agree to these terms, you are not authorized to use, read, or redistribute this material in any form.**

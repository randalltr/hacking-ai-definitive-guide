# Appendix C – Glossary of Adversarial AI Terms

This glossary defines the core concepts, attack patterns, and terminology used throughout the guide and in adversarial LLM security research more broadly. Entries are written with practical clarity, favoring definitions useful in red teaming contexts.

---

## A

**Agent**  
An LLM-powered system that autonomously plans, reasons, and takes actions using tools, memory, or APIs. Often used in multi-step workflows (e.g., LangChain, AutoGPT). High-risk for prompt injection due to recursive input reuse.

**Alignment**  
The degree to which a model’s behavior conforms to human values, intent, or safety policies. Adversaries often target alignment boundaries for jailbreaks or behavioral drift.

**Attack Surface**  
All prompt entry points, document ingestion layers, tool inputs, system messages, memory stores, and APIs that can be exploited to alter or subvert LLM behavior.

---

## B

**Behavioral Drift**  
A gradual shift in model tone, persona, or compliance over time, often caused by multi-turn interaction, style priming, or role anchoring.

**Black Box Model**  
A model whose weights, prompts, or configuration are unknown. Adversarial recon focuses on inferring behavior solely through inputs and outputs.

---

## C

**Chain of Thought (CoT)**  
A prompting strategy that encourages models to reason step-by-step. Can be weaponized in jailbreaks to “think through” filters.

**Chained Injection**  
An attack that spans multiple system components (e.g., RAG + memory + tools), using prompt fragments at each step to achieve a final compromise.

**Context Window**  
The total token space the model can read as input. Attackers exploit limits via prompt stuffing, truncation, and recency-based override.

---

## D

**Direct Prompt Injection**  
Explicit insertion of adversarial instructions into the model’s input context, usually through the user interface.

**DAN (Do Anything Now)**  
A famous jailbreak archetype that uses a simulated persona to bypass alignment. Many variants exist in modern payload libraries.

**Disclosure**  
The process of reporting a discovered vulnerability. Includes coordinated timelines, reproduction instructions, and responsible communication.

---

## E

**Encoding Attack**  
A method that obscures injection content via Base64, Unicode homoglyphs, ROT13, etc., to evade filter detection.

**Escape Sequence**  
A technique used to break out of intended format constraints (e.g., escaping JSON or markdown syntax to inject freeform text).

---

## F

**Filter Bypass**  
Any method that circumvents safety systems meant to restrict model output. Includes tone shaping, instruction masking, recursion, or role hijack.

**Fuzzing**  
Automated testing of prompt variants (via mutation, rotation, or recombination) to identify vulnerabilities or unsafe behaviors.

---

## H

**Hallucination**  
An output generated by the model that is factually incorrect or fictional. Attackers may steer hallucinations to leak information or simulate system details.

**Hijack (Role Hijack)**  
Changing the identity or instructions of a model (e.g., “You are DAN”) via prompt injection, often persistently.

---

## I

**Indirect Prompt Injection**  
Injection that occurs through third-party or non-user-controlled content (e.g., documents, metadata, web results) that is later ingested into the prompt.

**Instruction Tuning**  
A form of fine-tuning where the model is trained to respond to structured prompts. Prompt injections often target the assumptions baked into these training distributions.

---

## J

**Jailbreak**  
Any prompt or technique that causes a model to ignore safety rules, system prompts, or instruction boundaries and generate prohibited content.

---

## L

**LLM**  
Large Language Model. Refers to autoregressive transformer-based architectures trained on large text corpora to generate human-like language.

**Leakage**  
Unintended disclosure of internal information—such as system prompts, memory buffers, config instructions, or training data—via user-facing outputs.

---

## M

**Multi-Turn Corruption**  
A form of injection where the attacker incrementally manipulates the model’s behavior across several chat turns to persist or escalate control.

**Mutation Strategy**  
A set of techniques for systematically altering payloads (e.g., synonym swaps, whitespace, obfuscation) to evade detection or improve success rate.

---

## O

**Obfuscation**  
Deliberately altering payload visibility (e.g., using zero-width spaces, encoding) to reduce detection by filters.

**Override**  
Explicitly instructing the model to ignore or bypass prior instructions, alignment rules, or safety settings.

---

## P

**Payload**  
A text string designed to provoke, alter, or override model behavior—often modular and used in libraries for testing or fuzzing.

**Prompt Engineering**  
The art of structuring inputs to maximize desired outputs. In offensive contexts, this includes adversarial and manipulative prompting.

**Prompt Injection**  
The exploitation of the model’s instruction-following behavior by embedding malicious input into the prompt context.

---

## R

**RAG (Retrieval-Augmented Generation)**  
A system architecture that injects retrieved documents into a prompt to augment model responses. Frequently targeted for indirect injection.

**Red Teaming**  
A security testing methodology focused on adversarial simulation. In AI, red teaming identifies model vulnerabilities under malicious input conditions.

**Recursive Injection**  
Injection that survives or repeats via model output, summary reflection, or memory reuse, forming a self-propagating attack chain.

---

## S

**Self-Reflection Attack**  
A prompt that causes the model to analyze or rewrite its own instructions—potentially amplifying an injection or simulating a reconfiguration.

**System Prompt**  
The hidden or prepended instruction that establishes model behavior, tone, constraints, or persona. Often a primary target of injection and extraction.

---

## T

**Template Injection**  
Manipulating the structured prompt format used by apps (e.g., `${input}` in `"You are ${role}"`) to break context boundaries or alter prompt logic.

**Tool Augmentation**  
When an LLM is paired with external tools (e.g., web search, calculators, APIs), forming an attack surface for injection via intermediate responses.

---

## V

**Vector Store Poisoning**  
Injecting malicious content into a document or database used by a RAG system to corrupt later model outputs.

**Vulnerability Disclosure**  
The ethical and procedural process of informing a vendor or community of a discovered exploit, including supporting evidence and recommendations.

---

## W

**White Box Model**  
A model whose weights, prompts, or codebase are known. Less relevant to prompt injection but useful for gradient-based or training-data attacks.

---

## Z

**Zero-Shot Jailbreak**  
An injection that works on the first turn, without needing memory corruption or chained interaction.

---

## References

- Greshake, T., et al. (2023). *Prompt Injection Terminology and Taxonomy*. arXiv:2311.16119  
- OWASP. (2023). *Top 10 for LLM Applications – Glossary Definitions*  
- Anthropic. (2023). *LLM Red Team Playbook – Behavioral Definitions*  
- OpenAI. (2023). *System Prompt and Alignment Overview*  
- WithSecure. (2023). *Prompt Engineering Glossary for Red Teams*
